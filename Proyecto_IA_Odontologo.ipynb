{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea5c4409",
   "metadata": {},
   "source": [
    "# Alcance del Proyecto\n",
    "\n",
    "#### Descripción del Proyecto\n",
    "Desarrollar una IA generativa que pueda procesar, entender y responder preguntas relacionadas con la odontología a partir de un conjunto de documentos especializados. La IA deberá ser capaz de interpretar la información contenida en textos médicos, artículos de investigación, guías de tratamiento, entre otros, y proporcionar respuestas precisas y contextualmente relevantes.\n",
    "\n",
    "#### Funcionalidades Clave\n",
    "1. **Carga de Documentos**: La capacidad de cargar y almacenar documentos relacionados con la odontología en diversos formatos (PDF, Word, etc.).\n",
    "2. **Procesamiento de Lenguaje Natural (NLP)**: Utilizar técnicas avanzadas de NLP para entender y extraer información relevante de los documentos cargados.\n",
    "3. **Generación de Respuestas**: La capacidad de generar respuestas coherentes y precisas a preguntas específicas basadas en la información contenida en los documentos.\n",
    "4. **Interfaz de Usuario**: Desarrollar una interfaz amigable que permita a los usuarios cargar documentos y hacer preguntas a la IA.\n",
    "5. **Actualización y Mantenimiento**: Procedimientos para actualizar la base de datos de documentos y mejorar el modelo de IA con el tiempo.\n",
    "\n",
    "#### Restricciones y Limitaciones\n",
    "- **Idioma**: El proyecto se enfocará en documentos y consultas en español.\n",
    "- **Tipo de Información**: Limitado a documentos relacionados con la odontología.\n",
    "- **Privacidad y Seguridad**: Garantizar la seguridad y privacidad de los documentos cargados y la información procesada.\n",
    "\n",
    "### Objetivos del Proyecto\n",
    "\n",
    "#### Objetivo General\n",
    "Desarrollar una herramienta de IA generativa que pueda responder preguntas específicas sobre odontología, utilizando información extraída de documentos técnicos y científicos, para apoyar a profesionales de la salud dental en sus consultas y decisiones clínicas.\n",
    "\n",
    "#### Objetivos Específicos\n",
    "1. **Desarrollo del Sistema de Carga de Documentos**\n",
    "   - Implementar un módulo para cargar y almacenar documentos en diferentes formatos.\n",
    "   - Establecer un repositorio estructurado para la gestión de estos documentos.\n",
    "\n",
    "2. **Implementación del Procesamiento de Lenguaje Natural**\n",
    "   - Desarrollar modelos de NLP entrenados específicamente en terminología y conceptos odontológicos.\n",
    "   - Integrar técnicas de extracción de información para identificar y organizar datos relevantes de los documentos.\n",
    "\n",
    "3. **Desarrollo del Generador de Respuestas**\n",
    "   - Crear algoritmos de generación de respuestas que puedan utilizar la información procesada para responder preguntas con precisión.\n",
    "   - Asegurar que las respuestas sean coherentes y contextualmente adecuadas.\n",
    "\n",
    "4. **Diseño e Implementación de la Interfaz de Usuario**\n",
    "   - Diseñar una interfaz intuitiva para la interacción con la IA.\n",
    "   - Facilitar la carga de documentos y la formulación de preguntas por parte de los usuarios.\n",
    "\n",
    "5. **Evaluación y Mejora Continua**\n",
    "   - Realizar pruebas exhaustivas para evaluar la precisión y relevancia de las respuestas generadas.\n",
    "   - Implementar un sistema de feedback para mejorar continuamente la precisión del modelo de IA.\n",
    "\n",
    "### Metodología\n",
    "\n",
    "1. **Recopilación de Datos**: Reunir una amplia colección de documentos odontológicos de diversas fuentes.\n",
    "2. **Entrenamiento del Modelo**: Utilizar técnicas de aprendizaje automático y modelos pre-entrenados para adaptar la IA a la terminología y contextos específicos de la odontología.\n",
    "3. **Desarrollo e Integración**: Implementar las funcionalidades clave y asegurar la integración eficiente entre módulos.\n",
    "4. **Pruebas y Validación**: Probar el sistema con casos reales y ajustar el modelo según sea necesario.\n",
    "5. **Despliegue y Mantenimiento**: Desplegar la herramienta para su uso y establecer un plan de mantenimiento y actualización.\n",
    "\n",
    "### Herramientas y Tecnologías\n",
    "\n",
    "- **Procesamiento de Lenguaje Natural**: Herramientas y librerías como SpaCy, NLTK, y BERT.\n",
    "- **Almacenamiento y Gestión de Datos**: Bases de datos como MongoDB o PostgreSQL.\n",
    "- **Desarrollo de la Interfaz de Usuario**: Frameworks como React o Angular.\n",
    "- **Infraestructura**: Plataformas en la nube como AWS, Google Cloud o Azure para el despliegue y escalabilidad.\n",
    "\n",
    "Este plan proporciona una visión clara y estructurada del desarrollo de la IA generativa para odontología, asegurando que se aborden todos los aspectos cruciales del proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13653ea4",
   "metadata": {},
   "source": [
    "## Librerías utilizadas:\n",
    "\n",
    "- openai: Biblioteca que proporciona acceso a las API de OpenAI para utilizar modelos de lenguaje avanzados, como GPT-3, para generar texto natural y responder preguntas basadas en texto.\n",
    "\n",
    "- httpx: Cliente HTTP moderno para Python, utilizado para hacer solicitudes HTTP de manera eficiente y manejar respuestas.\n",
    "\n",
    "- PyPDF2: Biblioteca para trabajar con archivos PDF en Python. Permite leer, escribir y manipular documentos PDF.\n",
    "\n",
    "- langchain.text_splitter.CharacterTextSplitter: Parte de la biblioteca LangChain, que proporciona herramientas para dividir texto en fragmentos más pequeños, como párrafos o secciones, facilitando el procesamiento y análisis de grandes cantidades de texto.\n",
    "\n",
    "- langchain.embeddings.OpenAIEmbeddings: Herramienta dentro de LangChain que maneja embeddings o incrustaciones de texto generadas por OpenAI. Los embeddings son representaciones numéricas de texto que capturan su semántica y estructura.\n",
    "\n",
    "- langchain.vectorstores.FAISS: Herramienta de LangChain que utiliza FAISS (Facebook AI Similarity Search) para crear un vectorstore, que es una estructura de datos optimizada para búsqueda y recuperación rápida de información basada en vectores (como los embeddings de texto).\n",
    "\n",
    "- langchain.memory.ConversationBufferMemory: Componente de LangChain que gestiona la memoria de la conversación, almacenando el historial de interacciones pasadas del asistente virtual para mejorar la coherencia y relevancia de las respuestas futuras.\n",
    "\n",
    "- langchain.chains.ConversationalRetrievalChain: Implementación en LangChain de una cadena de conversación que combina un modelo de lenguaje conversacional (como ChatOpenAI), un vectorstore y una memoria de conversación para crear un sistema de diálogo que puede recuperar y utilizar contexto relevante.\n",
    "\n",
    "- langchain.chat_models.ChatOpenAI: Modelo de lenguaje conversacional específico de LangChain, utilizado aquí como parte de la configuración del sistema de conversación para interactuar con el usuario y generar respuestas basadas en el contexto y la entrada proporcionada.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "556aa2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librerias\n",
    "import openai\n",
    "import httpx\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8636021",
   "metadata": {},
   "source": [
    "## Función get_pdf_text(pdf_paths):\n",
    "\n",
    "- Descripción: Esta función recorre una lista de rutas de archivos PDF y extrae el texto completo de cada página de cada PDF.\n",
    "- Detalles: Utiliza PdfReader de PyPDF2 para leer cada página del PDF y extraer el texto. Cada documento extraído se guarda junto con metadatos que incluyen el nombre del documento y el número de página.\n",
    "- Salida: Devuelve una lista de diccionarios, donde cada diccionario contiene el texto extraído de un documento PDF y sus metadatos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "38da053f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para extraer texto de los PDF\n",
    "def get_pdf_text(pdf_paths):\n",
    "    documents = []\n",
    "    for doc_index, pdf_path in enumerate(pdf_paths):\n",
    "        pdf_reader = PdfReader(pdf_path)\n",
    "        for page_index, page in enumerate(pdf_reader.pages):\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                documents.append({\n",
    "                    \"text\": text,\n",
    "                    \"metadata\": {\n",
    "                        \"document\": pdf_path,\n",
    "                        \"page\": page_index + 1\n",
    "                    }\n",
    "                })\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e929bac",
   "metadata": {},
   "source": [
    "## Función get_text_chunks(documents):\n",
    "\n",
    "- Descripción: Divide el texto de los documentos en chunks más pequeños para facilitar el procesamiento y la búsqueda.\n",
    "- Detalles: Utiliza CharacterTextSplitter de langchain para dividir el texto en chunks de tamaño especificado (1000 caracteres por chunk, con un solapamiento de 200 caracteres). Cada chunk conserva los metadatos del documento y la página de donde se extrajo.\n",
    "- Salida: Devuelve una lista de diccionarios, donde cada diccionario contiene un chunk de texto y sus metadatos asociados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a4ab3e3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para dividir el texto en chunks\n",
    "def get_text_chunks(documents):\n",
    "    text_splitter = CharacterTextSplitter(\n",
    "        separator=\"\\n\\n\",  # Separar por párrafos\n",
    "        chunk_size=1000,\n",
    "        chunk_overlap=200,\n",
    "        length_function=len\n",
    "    )\n",
    "    \n",
    "    chunks = []\n",
    "    for document in documents:\n",
    "        text = document[\"text\"]\n",
    "        metadata = document[\"metadata\"]\n",
    "        page_chunks = text_splitter.split_text(text)\n",
    "        \n",
    "        for chunk in page_chunks:\n",
    "            chunks.append({\n",
    "                \"text\": chunk,\n",
    "                \"metadata\": metadata\n",
    "            })\n",
    "    \n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3aa381f",
   "metadata": {},
   "source": [
    "## Función get_vectorstore(text_chunks):\n",
    "\n",
    "- Descripción: Crea un vectorstore utilizando embeddings de OpenAI para representar los textos de los chunks en un espacio vectorial.\n",
    "- Detalles: Utiliza OpenAIEmbeddings para obtener embeddings de los textos y FAISS de langchain para crear un índice de búsqueda utilizando estos embeddings. Se incluyen metadatos para cada texto para poder recuperar información adicional.\n",
    "- Salida: Devuelve un vectorstore que permite realizar búsquedas eficientes basadas en similitud de texto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "310edd4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el vectorstore\n",
    "def get_vectorstore(text_chunks):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    texts = [chunk[\"text\"] for chunk in text_chunks]\n",
    "    metadatas = [chunk[\"metadata\"] for chunk in text_chunks]\n",
    "    vectorstore = FAISS.from_texts(texts=texts, embedding=embeddings, metadatas=metadatas)\n",
    "    return vectorstore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52e05c41",
   "metadata": {},
   "source": [
    "## Función para crear el asistente de odontología\n",
    "\n",
    "Esta función permite crear un asistente virtual especializado en odontología utilizando documentos en formato PDF como base de conocimiento.\n",
    "\n",
    "1. Extracción y procesamiento de textos PDF:\n",
    "\n",
    "- get_pdf_text(pdf_paths): Extrae el texto de los archivos PDF especificados en pdf_paths y organiza la información en documentos estructurados que incluyen el texto extraído y metadatos como el nombre del documento y el número de página.\n",
    "\n",
    "2. División del texto en chunks:\n",
    "\n",
    "- get_text_chunks(documents): Divide el texto de cada documento en fragmentos más pequeños llamados \"chunks\", utilizando un separador para delimitar párrafos. Esto facilita el manejo y la búsqueda de información más específica dentro de cada documento.\n",
    "Creación del vectorstore:\n",
    "\n",
    "- get_vectorstore(text_chunks): Crea un vectorstore utilizando los chunks de texto procesados. Se utiliza FAISS para generar representaciones vectoriales de los textos, empleando embeddings de OpenAI para capturar la semántica y similitudes entre los chunks.\n",
    "\n",
    "3. Configuración del modelo de lenguaje y la memoria:\n",
    "\n",
    "- ChatOpenAI() y ConversationBufferMemory(): Configura un modelo de lenguaje basado en ChatGPT y una memoria para mantener el historial de la conversación. Esto permite al asistente mantener coherencia y contexto durante interacciones sucesivas.\n",
    "\n",
    "4. Creación de la cadena de conversación:\n",
    "\n",
    "- ConversationalRetrievalChain.from_llm(...): Establece una cadena de conversación que integra el modelo de lenguaje, el vectorstore y la memoria. Esto habilita al asistente para recuperar y utilizar contextos relevantes basados en los textos extraídos de los documentos PDF.\n",
    "\n",
    "5. Función para interactuar con el asistente:\n",
    "\n",
    "- odontology_assistant(query): Es una función interna que permite realizar consultas al asistente de odontología. Toma como entrada una pregunta (query), la envía a la cadena de conversación configurada y recibe una respuesta. La respuesta se formatea para incluir el texto de la respuesta generada y las referencias de los documentos y páginas más relevantes según el contexto recuperado.\n",
    "\n",
    "6. Retorno del asistente de odontología:\n",
    "\n",
    "- Devuelve la función odontology_assistant, lista para ser utilizada para interactuar con el asistente virtual creado. Esta función encapsula toda la configuración y lógica necesaria para procesar consultas relacionadas con odontología basadas en los documentos PDF proporcionados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b59ded8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función para crear el asistente de odontología\n",
    "def create_odontology_assistant(pdf_paths):\n",
    "    # Extraer y procesar el texto de los PDFs\n",
    "    documents = get_pdf_text(pdf_paths)\n",
    "    text_chunks = get_text_chunks(documents)\n",
    "    vectorstore = get_vectorstore(text_chunks)\n",
    "    \n",
    "    # Configurar el LLM y la memoria de la cadena de conversación\n",
    "    llm = ChatOpenAI()\n",
    "    memory = ConversationBufferMemory(memory_key='chat_history', return_messages=True)\n",
    "    \n",
    "    # Crear la cadena de conversación con recuperación de contexto\n",
    "    conversation_chain = ConversationalRetrievalChain.from_llm(\n",
    "        llm=llm,\n",
    "        retriever=vectorstore.as_retriever(),\n",
    "        memory=memory\n",
    "    )\n",
    "    \n",
    "    # Función para interactuar con el asistente\n",
    "    def odontology_assistant(query):\n",
    "        inputs = {\"question\": query}\n",
    "        response = conversation_chain(inputs)\n",
    "        \n",
    "        # Obtener los metadatos y texto de los chunks más similares\n",
    "        most_similar_chunks = response.get(\"source_documents\", [])\n",
    "        chunks_info = \"\"\n",
    "        for chunk in most_similar_chunks:\n",
    "            document = chunk.metadata.get(\"document\", \"Desconocido\")\n",
    "            page = chunk.metadata.get(\"page\", \"Desconocida\")\n",
    "            chunk_text = chunk.text\n",
    "            chunks_info += f\"\\n\\nChunk:\\n{chunk_text}\\nDocumento: {document}, Página: {page}\\n\"\n",
    "        \n",
    "        # Formatear la respuesta final con referencias a los chunks más similares\n",
    "        formatted_response = f\"{response['answer']}\\n\\nBasado en los chunks más similares:{chunks_info}\"\n",
    "        \n",
    "        return formatted_response\n",
    "    \n",
    "    return odontology_assistant\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189aece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la API key desde las variables de entorno\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# Paso 1: Leer los PDF y extraer la información\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            print(f\"Procesando archivo: {file_path}\")\n",
    "            try:\n",
    "                reader = PdfReader(file_path)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                pdf_texts[file_name] = text\n",
    "                print(f\"Texto extraído del archivo {file_name}: {text[:500]}...\")  # Muestra los primeros 500 caracteres\n",
    "            except Exception as e:\n",
    "                print(f\"Error procesando {file_name}: {e}\")\n",
    "    return pdf_texts\n",
    "\n",
    "# Paso 2: Dividir la información en chunks\n",
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    print(f\"Dividido el texto en {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Paso 3: Crear el índice de búsqueda\n",
    "def create_faiss_index(pdf_texts):\n",
    "    print(\"Creando embeddings e índice FAISS\")\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    for file_name, text in pdf_texts.items():\n",
    "        chunks = split_text_into_chunks(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            texts.append(chunk)\n",
    "            metadatas.append({'document': file_name, 'chunk': i})\n",
    "    index = FAISS.from_texts(texts, embeddings, metadatas)\n",
    "    print(\"Índice FAISS creado\")\n",
    "    return index\n",
    "\n",
    "# Paso 4: Generar una respuesta\n",
    "def generate_response(query, index, top_k=5):\n",
    "    print(f\"Generando respuesta para la consulta: {query}\")\n",
    "    docs_and_scores = index.similarity_search_with_score(query, k=top_k)\n",
    "    \n",
    "    relevant_chunks = \"\"\n",
    "    for doc, score in docs_and_scores:\n",
    "        relevant_chunks += f\"\\n\\nChunk {doc.metadata['chunk']} del documento {doc.metadata['document']}:\\n{doc.page_content}\"\n",
    "    \n",
    "    # Usar el modelo de OpenAI para generar una respuesta basada en los chunks relevantes\n",
    "    llm = OpenAI()\n",
    "    prompt = f\"Responde a la siguiente pregunta basándote en la información proporcionada:\\n\\nPregunta: {query}\\n\\nInformación:\\n{relevant_chunks}\\n\\nRespuesta:\"\n",
    "    response = llm(prompt)\n",
    "\n",
    "    final_response = f\"\"\"Response: {response.strip()}\\n\\nRELEVANT CHUNKS:{relevant_chunks}\"\"\"\n",
    "    return final_response\n",
    "\n",
    "def main():\n",
    "    folder_path = './documentacion_odontologia'  # Cambia esto por la ruta a tu carpeta de PDFs\n",
    "    print(\"Extrayendo textos de PDFs...\")\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "    print(\"Textos extraídos de todos los PDFs\")\n",
    "\n",
    "    index = create_faiss_index(pdf_texts)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Haz una pregunta: \")\n",
    "        response = generate_response(query, index)\n",
    "        print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f78ca0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Extrayendo textos de PDFs...\n",
      "INFO:__main__:Procesando archivo: C:/Users/jaime/OneDrive/Escritorio/Proyectos/Proyectos_IA/documentacion_odontologia\\COMPENDIOENDODONCIA2016.pdf\n",
      "INFO:__main__:Texto extraído del archivo COMPENDIOENDODONCIA2016.pdf: See discussions, st ats, and author pr ofiles f or this public ation at : https://www .researchgate.ne t/public ation/303961195\n",
      "COMPENDIO DE ENDODONCIA\n",
      "Research · June 2016\n",
      "DOI: 10.13140/RG.2.1.1772.5041\n",
      "CITATIONS\n",
      "0READS\n",
      "71,449\n",
      "2 author s:\n",
      "Javier Alv arez Rodrígue z\n",
      "Univ ersidad de Ciencias Médic as de La Hab ana\n",
      "107 PUBLICA TIONS    156 CITATIONS    \n",
      "SEE PROFILE\n",
      "Dachel Martíne z Asanz a\n",
      "National School of Public He alth, Hav ana Medic al Scienc es Univ ersity\n",
      "56 PUBLICA TIONS    119 CITATIONS  ...\n",
      "INFO:__main__:Procesando archivo: C:/Users/jaime/OneDrive/Escritorio/Proyectos/Proyectos_IA/documentacion_odontologia\\Odontologia_sanitaria.pdf\n",
      "INFO:__main__:Texto extraído del archivo Odontologia_sanitaria.pdf: ODONTOLOGIA\n",
      "SANITARIAAgradecimiento\n",
      "La Oficina Sanitaria Panamericana agradece a la Fundación W. K.\n",
      "Kellogg, su generosa ayuda al sufragar los costos de la impresión de este\n",
      "libro.\n",
      "Agradece igualmente a los odontólogos sanitarios, cuyos nombres aparecen\n",
      "al pie de la primera página de cada capítulo, su gentil y valiosa contribución\n",
      "en la traducción del libro.ODONTOLOGIA SANITARIA\n",
      "Dr. MARIO M. CHAVES\n",
      "Asesor Regional en Odontologta Sanitaria\n",
      ",-( u w.1\n",
      "Publicaciones Científicas\n",
      "No. 63Julio de 1962\n",
      "O...\n",
      "INFO:__main__:Procesando archivo: C:/Users/jaime/OneDrive/Escritorio/Proyectos/Proyectos_IA/documentacion_odontologia\\Odontopediatria_Adeslas_Dental.pdf\n",
      "INFO:__main__:Texto extraído del archivo Odontopediatria_Adeslas_Dental.pdf: Cuida de la boca de tus hijos/as y\n",
      "anticípate a cualquier pr oblema en sus\n",
      "dientes.ODONTOPEDIATRÍA\n",
      "ESTE TRATAMIENT O ES PARA TI SI…\n",
      "SEAS O NO DE ADESLAS\n",
      "Precios Pide tu primer a cita+¿Crees que tu hijo/a necesita ortodoncia? HAZ EL  TEST¿Qué es la odont opediatría?\n",
      "La odont opediatría es la r ama de la odont ología que se encar ga del diagnóstico y\n",
      "tratamient o de las enf ermedades dentales infantiles  desde la tempr ana infancia\n",
      "hasta la entr ada en la puber tad.\n",
      "El objetiv o de los tr atamient...\n",
      "INFO:__main__:Procesando archivo: C:/Users/jaime/OneDrive/Escritorio/Proyectos/Proyectos_IA/documentacion_odontologia\\Tesis_Carga_Microbiana.pdf\n",
      "INFO:__main__:Texto extraído del archivo Tesis_Carga_Microbiana.pdf:   \n",
      " \n",
      " \n",
      "FACULTAD DE   MEDICINA   HUMANA  Y  CIENCIAS   DE  LA   \n",
      "SALUD ESCUELA  PROFESIONAL DE ESTOMATOLOGÍA  \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "CARGA MICROBIANA EN LIGAS ELÁSTICAS DE DOS \n",
      "MARCAS UTILIZADAS EN ADOLESCENTES  PORTADORES \n",
      "DE BRACKETS  DEL CONSULTORIO  ODONTOLÓGICO SAN  \n",
      "NICOLÁS.  AREQUIPA -2017 . \n",
      " \n",
      " \n",
      " \n",
      " \n",
      " \n",
      "                                                                 Tesis Presentada  por el Bachiller : \n",
      "ALAN ROBERT MOGROVEJO ALVAREZ  \n",
      "para optar el Título Profesional de  \n",
      "Cirujano Dentista  \n",
      " \n",
      " \n",
      " \n",
      "AREQU...\n",
      "INFO:__main__:Textos extraídos de todos los PDFs\n",
      "INFO:__main__:Creando embeddings e índice FAISS\n",
      "INFO:__main__:Dividido el texto en 498 chunks\n",
      "INFO:__main__:Dividido el texto en 1856 chunks\n",
      "INFO:__main__:Dividido el texto en 12 chunks\n",
      "INFO:__main__:Dividido el texto en 148 chunks\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:__main__:Índice FAISS creado\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Haz una pregunta: Cual es el mejor tratamiento para las caries?\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:__main__:Generando respuesta para la consulta: Cual es el mejor tratamiento para las caries?\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/embeddings \"HTTP/1.1 200 OK\"\n",
      "INFO:httpx:HTTP Request: POST https://api.openai.com/v1/completions \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response: De acuerdo a la información proporcionada, el mejor tratamiento para las caries es la utilización del flúor en métodos como la fluoruración del agua y las aplicaciones tópicas. Otros métodos como el uso de flúor en comprimidos, la fluoruración de la sal de cocina y la pasta dentífrica conteniendo fluoruro también son alternativas prometedoras, pero aún no completamente demostradas. Además, se recomienda una nutrición adecuada para fortalecer los dientes y evitar el ataque de la caries. Finalmente, se menciona la importancia de realizar un examen periódico y un tratamiento de lesiones en su inicio como medidas de prevención y diagnóstico temprano.\n",
      "\n",
      "RELEVANT CHUNKS:\n",
      "\n",
      "Chunk 451 del documento Odontologia_sanitaria.pdf:\n",
      " caries.\n",
      "De momento, nuestros mejores métodos de prevención de la caries dental\n",
      "son aquellos que se relacionan con el mecanismo de defensa, más específica-\n",
      "mente, con la utilización del flúor. Son en.verdad inmensas las posibilidades\n",
      "que el flúor ofrece al odontólogo sanitario y apenas han sido parcialmente\n",
      "estudiadas. A la hora en que escribimos este capítulo podemos decir que\n",
      "disponemos de dos métodos establecidos de utilización del flúor: la fluorura-\n",
      "ción del agua y las aplicaciones tópicas. Existen alternativas prometedoras,\n",
      "pero aún no completamente demostradas: el uso del flúor en comprimidos,\n",
      "la fluoruración de la sal de cocina y la pasta dentífrica conteniendo fluoruro\n",
      "estafioso. Otras alternativas que parecen ofrecer menos posibilidades, se\n",
      "relacionan con el suplemento de flúor a través de la leche mediante el\n",
      "enriquecimiento de las harinas.\n",
      "Como queremos ceñirnos a los métodos más importantes, tanto desde el\n",
      "punto de vista de la protección ofrecida como de la posibilidad de \n",
      "\n",
      "Chunk 436 del documento Odontologia_sanitaria.pdf:\n",
      "\n",
      "siado, dejaremos esa tarea a cargo del lector.Individual\n",
      "52\n",
      "Paciente-auxiliar\n",
      "42\n",
      "Paciente-profesional\n",
      "32\n",
      "Acción gubernamental\n",
      "restringida\n",
      "22\n",
      "Acción gubernamental amplia\n",
      "12\n",
      "Aplicación 12 22 32 42 52\n",
      "FOMENTO DE PRO DIAGNOSTICO Y LMTACON\n",
      "NIVELES I PREVENCION LA SALUD ESPECIFICA TRATAMIENTO DEL DAÑO REALTCONPRECOCES146CARIES DENTAL\n",
      "En la lucha contra el problema de la caries dental, han sido empleados\n",
      "o indicados los siguientes métodos:\n",
      "1or Nivel de prevención\n",
      "(fomento de la salud)\n",
      "Nutrición adecuada en el período de\n",
      "formación de los dientes\n",
      "Se trata en realidad de un método genérico dirigido hacia el logro y\n",
      "mantenimiento de la salud plena del individuo y, por lo tanto, de la salud\n",
      "oral como parte integrante de aquélla. Los dientes bien formados deben\n",
      "resistir necesariamente mejor el ataque de la caries dental. A pesar de dicha\n",
      "generalización, se ha podido establecer la existencia de una relación perfecta-\n",
      "mente definida entre la nutrición y la caries dental, determinada por la\n",
      "presencia\n",
      "\n",
      "Chunk 499 del documento Odontologia_sanitaria.pdf:\n",
      "nfirman lo que dijimos an-\n",
      "teriormente sobre las limitaciones de dicho método para su utilización en\n",
      "gran escala (24).\n",
      "Queremos finalizar nuestro examen de los métodos de 20 nivel (protección\n",
      "específica), repitiendo una frase que oimos a Jay: \"Hoy sólo tiene caries165ODONTOLOGIA SANITARIA\n",
      "quien quiere\". En realidad, combinando lo que hoy sabemos sobre la\n",
      "prevención de la caries, aumentando la resistencia del diente mediante el\n",
      "suministro de flúor, atenuando el ataque con el control efectivo del azúcar,\n",
      "y haciendo uso juiciosamente de las medidas de 1er nivel (promoción de\n",
      "la salud), llegaríamos prácticamente a un 100% de reducción en la inci-\n",
      "dencia de la caries. Nuestro problema es, pues, el de aplicar en la forma\n",
      "más extensa posible los conocimientos que hoy poseemos, sin dejar por eso\n",
      "de seguir buscando otros métodos más eficaces y de aplicación más simple.\n",
      "3cr Nivel de prevención\n",
      "(diagnóstico y tratamiento precoces)\n",
      "Es un nivel de suma importancia en relación con el problema de la \n",
      "\n",
      "Chunk 500 del documento Odontologia_sanitaria.pdf:\n",
      "e seguir buscando otros métodos más eficaces y de aplicación más simple.\n",
      "3cr Nivel de prevención\n",
      "(diagnóstico y tratamiento precoces)\n",
      "Es un nivel de suma importancia en relación con el problema de la caries\n",
      "dental, ya que como vimos en la parte referente a las necesidades de trata-\n",
      "miento, éstas crecen en proporción geométrica, en tanto que el tiempo de\n",
      "desatención crece en forma aritmética. En el caso de la caries dental, los\n",
      "dos métodos de que disponemos envuelven un trabajo reparador, restaurador\n",
      "de las lesiones producidas por la caries. El primero de ellos sería el trata-\n",
      "miento de lesiones en su inicio, extendiendo los límites de la cavidad hasta\n",
      "las zonas de la superficie dental con resistencia natural a la caries (extensión\n",
      "preventiva). Para descubrir las lesiones en su inicio, es indispensable un\n",
      "examen periódico a intervalos cortos, principalmente durante las épocas de\n",
      "la vida en que el ataque es más intenso. Este método, diagnóstico y trata-\n",
      "miento de lesiones en su inicio, o\n",
      "\n",
      "Chunk 440 del documento Odontologia_sanitaria.pdf:\n",
      "se obtiene una reducción significativa en la inci-\n",
      "dencia de la caries, pasaremos entonces este método para el 20 nivel. Ni\n",
      "aun considerando el aspecto de la frecuencia y ocasión (hasta 10 minutos\n",
      "después de las comidas) en que se hace el cepillado, encontramos en verdad\n",
      "suficiente demostración para poder situar este método entre los de protec-\n",
      "ción específica, aunque existan resultados favorables (2). Estos métodos\n",
      "tienen su base en datos experimentales, que indican que la caries resulta\n",
      "de un proceso aditivo de ataque al esmalte, cada vez que el pH de la placa\n",
      "cae bajo un cierto nivel crítico y durante pocos minutos después de la\n",
      "ingestión de azúcares, siendo el pH neutralizado después, por el poder\n",
      "neutralizador de la saliva.\n",
      "Por ahora, preferimos considerar los hábitos de higiene oral, incluyendo\n",
      "el cepillado y desde el punto de vista de la caries dental, como un método\n",
      "de fomento de la salud.*\n",
      "Oclusión normal\n",
      "No cabe duda de que una mala oclusión dificulta la masticación y la\n",
      "auto\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.llms import OpenAI\n",
    "import logging\n",
    "\n",
    "# Configuración de logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Cargar las variables de entorno\n",
    "load_dotenv()\n",
    "\n",
    "# Obtener la API Key desde las variables de entorno\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if openai_api_key is None:\n",
    "    raise ValueError(\"La API Key de OpenAI no se ha encontrado. Por favor, verifica tu archivo .env\")\n",
    "\n",
    "# Funciones de procesamiento\n",
    "def extract_text_from_pdfs(folder_path):\n",
    "    pdf_texts = {}\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if file_name.endswith('.pdf'):\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            logger.info(f\"Procesando archivo: {file_path}\")\n",
    "            try:\n",
    "                reader = PdfReader(file_path)\n",
    "                text = \"\"\n",
    "                for page in reader.pages:\n",
    "                    text += page.extract_text()\n",
    "                pdf_texts[file_name] = text\n",
    "                logger.info(f\"Texto extraído del archivo {file_name}: {text[:500]}...\")  # Muestra los primeros 500 caracteres\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error procesando {file_name}: {e}\")\n",
    "    return pdf_texts\n",
    "\n",
    "def split_text_into_chunks(text, chunk_size=1000, overlap=200):\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(text):\n",
    "        end = min(start + chunk_size, len(text))\n",
    "        chunks.append(text[start:end])\n",
    "        start += chunk_size - overlap\n",
    "    logger.info(f\"Dividido el texto en {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "def create_faiss_index(pdf_texts):\n",
    "    logger.info(\"Creando embeddings e índice FAISS\")\n",
    "    embeddings = OpenAIEmbeddings(openai_api_key=openai_api_key)\n",
    "    texts = []\n",
    "    metadatas = []\n",
    "    for file_name, text in pdf_texts.items():\n",
    "        chunks = split_text_into_chunks(text)\n",
    "        for i, chunk in enumerate(chunks):\n",
    "            texts.append(chunk)\n",
    "            metadatas.append({'document': file_name, 'chunk': i})\n",
    "    index = FAISS.from_texts(texts, embeddings, metadatas)\n",
    "    logger.info(\"Índice FAISS creado\")\n",
    "    return index\n",
    "\n",
    "def generate_response(query, index, top_k=5):\n",
    "    logger.info(f\"Generando respuesta para la consulta: {query}\")\n",
    "    docs_and_scores = index.similarity_search_with_score(query, k=top_k)\n",
    "    \n",
    "    relevant_chunks = \"\"\n",
    "    for doc, score in docs_and_scores:\n",
    "        relevant_chunks += f\"\\n\\nChunk {doc.metadata['chunk']} del documento {doc.metadata['document']}:\\n{doc.page_content}\"\n",
    "    \n",
    "    llm = OpenAI(api_key=openai_api_key)\n",
    "    prompt = f\"Responde a la siguiente pregunta basándote en la información proporcionada:\\n\\nPregunta: {query}\\n\\nInformación:\\n{relevant_chunks}\\n\\nRespuesta:\"\n",
    "    response = llm(prompt)\n",
    "\n",
    "    final_response = f\"\"\"Response: {response.strip()}\\n\\nRELEVANT CHUNKS:{relevant_chunks}\"\"\"\n",
    "    return final_response\n",
    "\n",
    "def main():\n",
    "    folder_path = './documentacion_odontologia'  # Cambia esto por la ruta a tu carpeta de PDFs\n",
    "    logger.info(\"Extrayendo textos de PDFs...\")\n",
    "    pdf_texts = extract_text_from_pdfs(folder_path)\n",
    "    logger.info(\"Textos extraídos de todos los PDFs\")\n",
    "\n",
    "    index = create_faiss_index(pdf_texts)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Haz una pregunta: \")\n",
    "        response = generate_response(query, index)\n",
    "        print(response)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f95700",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
